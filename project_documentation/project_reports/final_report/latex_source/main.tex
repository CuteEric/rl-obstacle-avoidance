\documentclass{article}
% \usepackage[utf8]{inputenc}
\usepackage{titling}
\usepackage{amsmath,amssymb}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{subfig}

\makeatletter
\newcommand*{\centerfloat}{%
  \parindent \z@
  \leftskip \z@ \@plus 1fil \@minus \textwidth
  \rightskip\leftskip
  \parfillskip \z@skip}
\makeatother

\title{Applied Reinforcement Learning\\
        \vspace{-0.25cm}
        {\large SS 2019}\\
        \vspace{0.25cm}
        {Project Final Report --- Obstacle Avoidance}}

\author{
  \small Akbar, Uzair\\
  \small \texttt{3697290}
  \and
  \small G\"undogan Alperen\\
  \small \texttt{3694565}
  \and
  \small Ellouze, Rachid \\
  \small \texttt{3671114}
}

\date{July 16, 2019}

\begin{document}
\begin{titlingpage}
    \maketitle
    \begin{abstract}
This report presents the final status for the TUM Applied Reinforcement Learning class project (Summer Semester 2019). We have successfully completed implementation of the Q-learning and SARSA algorithms both with and without LVFA for obstacle avoidance using LIDAR ranging data. The agent is tested in a Stage simulator environment and selected results are presented herein. After the training of the agent, the learned policy is transferred to the real turtlebot to test and we have obtained also successful results for the real turtlebot.
    \end{abstract}
    \tableofcontents
\end{titlingpage}
% \section*{Abstract}

\section{Introduction \& Project Objectives}
We start off by quickly reiterating our problem statement and solution objectives. We intend to implement a solution for the obstacle avoidance problem for a turtlebot robot using a LIDAR sensor. The two metrics (which we shall try to quantify in the form of a reward function in section 3.3) to evaluate the quality of our solution are:
\begin{itemize}
\item The turtlebot covers maximum distance within an episode.
\item The turtlebot avoids collisions.
\end{itemize}

\section{Environment Setup}
\begin{figure}[ht]
 \centering
 \includegraphics[width=.8\textwidth]{stage.png}
  \caption{Turtlebot in the Stage simulator environment.}
  \label{fig:stage}
\end{figure}

As discussed in our project proposal report, we chose to implement our Reinforcement Learning (RL) based obstacle avoidance solution first in a simulation environment, mainly for quick prototyping purposes. We shall later generalize to the real world using prospective approaches as listed in the proposal report.

We used the Stage simulator as a training environment as shown in Figure \ref{fig:stage}. Because the LIDAR sensor that is provided on the turtlebot does ranging estimates in a horizontal, 2D-plane, it was preferable to use a 2D simulator such as Stage so as to reduce unnecessary computational overhead. Domain randomization technique is exploited to generalize the policy during training and close the gap between simulation and reality. Our RL agent subsequently starts receiving LIDAR ranging data over \ang{360} with a resolution of \ang{1}. This data is pre-processed by a simple filter which observes last N consecutive lidar measurements to replace unreliable laser hits of the last measurement with the reliable ones from the previous measurements. It is expected to enhance the performance of the real turtlebot with this filter. The received filtered scan data then is used to model our problem as an MDP to be solved by our RL-agent. Then, the trained policy in the simulation environment is transferred to real turtlebot 

\section{The RL Agent}
We implemented our MDP as described in our project proposal report. However, we restate some of the details here because they have either been changed slightly or were not as clearly communicated in the project proposal report.

\begin{figure}[ht]
 \centering
 \includegraphics[width=.4\textwidth]{lidar_discretization.pdf}
  \caption{Discretization of LIDAR.}
  \label{fig:stage}
\end{figure}

\begin{figure}[ht]
 \centering
 \includegraphics[width=.3\textwidth]{action_space.pdf}
  \caption{Action Space.}
  \label{fig:stage}
\end{figure}
        
\subsection{State Space Definition}
Similar to our proposal, we define the state-space as $\mathcal{S} := \big\{ s\,\big\vert \, s= [\,d^T\:,\,v\,,\,w\,]^T \big\}$, where $d$ is the depth vector observed from the on-board LIDAR sensor. To leverage temporal information, $v$ and $w$ are linear and angular velocities of the robot respectively and are discretized as
\begin{equation*}
  v\in V := \big\{ 0.4, 0.2\,\si{m.s^{-1}}\big\}, \;\;\;w\in W:=\big\{ \frac{\pi}{6}, \frac{\pi}{12}, 0, -\frac{\pi}{12}, -\frac{\pi}{6}\,\si{rad.s^{-1}}\big\}.
\end{equation*}
Similarly, we also discretize $d$ into angular sectors as well as radially (details in section 4), to achieve a final vector of size of $5 \times 6 = 36$ (unless stated otherwise), enumeration over all possible values of which gives $5^6$. Hence, in total, our state space consists of $5^6 * 2 * 5 = 156250$ discrete states.

\subsection{Action Space Definition}
The control space $\mathcal{A}$ shall determine the permissible linear and angular velocities for the turtlebot. We explored two possible discretizations of the action space:
\begin{enumerate}
    \item $\mathcal{A}_1 := \Big\{ a \;\Big\vert\; a = \big(v', w'\big), \;v'\in V, \;w'\in W \Big\}$, where $v'$ and $w'$ are next state linear and angular velocities of the robot respectively. Hence, $\vert \mathcal{A}_1 \vert = 10$. This simply means that the current action determines what the velocity part of the next state should be.
    \item A simpler/smaller action space where $v'$ and $w'$ are ``decoupled,'' --- $\mathcal{A}_2 := \big\{ a\; \big\vert \;a \in \mathcal{A}_1\,,\: v' = v \,\text{or}\,\; w' = w \big\}$, with $\vert \mathcal{A}_2 \vert = 7$. 
\end{enumerate}

\subsection{The Reward Function}
Our task for the turtlebot is for it to go as fast as possible without any collisions. For this, we can employ a simple, action agnostic stage-wise reward function
\begin{equation*}
  r(s) =
  \begin{cases}
    v \cdot \cos(w) \cdot \delta t & \text{if episode running,} \\
    -10 & \text{if collision.}
  \end{cases}
\end{equation*}
The cosine term penalizes meaningless rotation, and $\delta t$ is the time-step for our turtlebot (set to 0.14 s).

\section{Preliminary Results}
\subsection{General Training Configuration}
The following setting should be taken as default in all of our experiments unless stated otherwise for comparitive study purposes.
\begin{itemize}
\item {\bf State Space:} The LIDAR sensor delivers readings over \ang{360} degrees (\ang{1} precision) of range measurements up to 6.5 meters. The original LIDAR space is then $[\ang{0}, \ang{360}) \times [0, 6.5m[$. We have discretized the two dimensions separately. Range values are divided radially into 5 levels --- $[0, 0.5[$, $[0.5, 1[$, $[1, 1.5[$, $[1.5, 2.5[$ and $[2.5, 6.5[$. The intuition behind higher resolution in near-range regions is that it is very critical to select an action when the robot is close to an obstacle. For angles, we limited ourselves to $[\ang{-60}, \ang{60}]$ interval for now, which is divided 6 equal sectors.
\item {\bf Action Space: } Both, linear and angular velocities are not obtained directly from the environment. They are read directly from the previously taken action, as defined by action space $\mathcal{A}_2$. This presents a way to encode the previous states, as the MDP do not allow history dependent policies.
\item {\bf RL Algorithm:} We use simple Q-learning with eligibility traces and $\epsilon$-greedy behaviour policy and enumerate over the states using a look-up table.
\end{itemize}

\subsection{Experiments}
In order to find (or validate) reasonable values for the above mentioned default configurations, we conducted the preliminary following experiments, all of which have been performed 5 times over 250 episodes and the results are aggregated and plotted in Figure 2 as mean and standard-deviation of the cumulative rewards of that respective episode.
\begin{itemize}
\item {\bf State Space:} We compared the default LIDAR discretization strategy with lower radial discretization of only the 3 lower levels and compared the cumulative rewards per episode in Figure 2 Part (a). Similarly, we compared the sector-wise angular discretization with 3 and 10 equal radial segmentation as shown in Figure 2 Part (b).
\item {\bf Action Space: } Both, $\mathcal{A}_1$ and $\mathcal{A}_2$ are compare in Figure 2 Part (d), and, as expected, $\mathcal{A}_2$ outperforms $\mathcal{A}_1$ on at least the rate of learning (the quality of solution for higher epochs remain to be seen).
\item {\bf Eligibility Traces:} For completeness purposes (and more for debugging purposes), we compared the results of eligibility traces implementation with that of no eligibility traces, and as expected got better performance for the former.
\end{itemize}








\begin{figure}[p]
\centerfloat
\begin{tabular}{c c}
\subfloat[Radial resolution of LIDAR readings.]{\includegraphics[width=0.49\textwidth]{rangeLevels.pdf}\label{fig:a}}
&
\subfloat[Angular resolution of LIDAR readings.]{\includegraphics[width=0.49\textwidth]{sectorLevels.pdf}\label{fig:b}}
\\
\subfloat[Eligibility traces vs. no eligibility traces.]{\includegraphics[width=0.49\textwidth]{eligibilityTrace.pdf}\label{fig:c}}
&
\subfloat[Comparing action spaces $\mathcal{A}_1$ \& $\mathcal{A}_2$.]{\includegraphics[width=0.49\textwidth]{actionSpace2.pdf}\label{fig:d}}
\end{tabular}
\caption[]{Preliminary results for comparison of different training settings.}
\label{fig:results}
\end{figure}

\section{Future Works}
\begin{itemize}
\item Test the implemented algorithms in real environment and tune the paramters to improve the results.
\item Implement better feature extraction method e.g. autoencoder.
\item Implement SARSA with LVFA on $\epsilon$ - greedy policy. 
\item Implement better action space and reward signals.
\item Implement and compare sof-tmax policy with $\epsilon$-greedy.
\item Determine the final approach and parameters to be used in the real turtlebot.
\end{itemize}
\medskip
% \bibliographystyle{ieeetr}
% \bibliography{references}



\end{document}
