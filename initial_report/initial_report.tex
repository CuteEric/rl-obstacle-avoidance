% PLEASE FILL IN THE PLACEHOLDERS <...>
%
% Diplomarbeit/Studienarbeit/IDP von <NAME>
% Diploma thesis of <NAME>
%
% Title: <TITLE>
%        <TITLE>
%
\documentclass[12pt,a4paper]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% PACKAGES:
% Define typearea
% a) Use automatic:
\usepackage[BCOR1cm]{typearea}

% Use German :
\usepackage[german, english]{babel}
% Use list of tabels, etc. in table of contents:
\usepackage{tocbibind}
% German paragraph skip
\usepackage{parskip}
% Encoder:????
%\usepackage[utf-8]{inputenc}
\usepackage[utf8]{inputenc}
% Use A4-paper efficiently:
\usepackage{a4wide}
% Index-generation
\usepackage{makeidx}
% Einbinden von URLs:
\usepackage{url}
% Include .eps-files (needed also for the LKN-logo):
%\usepackage{epsf}
\usepackage{epsfig}
\usepackage{epstopdf}
% Special \LaTex symbols (e.g. \BibTeX):
\usepackage{doc}
% Include Graphic-files:
%\usepackage{graphics}
% Include Graphic-files:
\usepackage{graphicx}
\usepackage{pgfgantt}
% Include doc++ generated tex-files:
%\usepackage{docxx}
% Include PDF links
\usepackage[pdftex, bookmarks=true]{hyperref}

\definecolor{barblue}{RGB}{153,204,254}
\definecolor{groupblue}{RGB}{51,102,254}
\definecolor{linkred}{RGB}{165,0,33} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% OTHER SETTINGS:

% Pagestyle:
\pagestyle{headings}

% Avoid 'overhang':
\sloppy

% Choose language
\newcommand{\setlang}[1]{\selectlanguage{#1}\nonfrenchspacing}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% TITLE:

\begin{document}


\thispagestyle{empty}
\newpage

\vspace{5cm}

\parbox{15cm}{\begin{center} {\sf\bf 
                               \Large  Technische Universität München
                                \smallskip

                               \Large Lehrstuhl für Datenverarbeitung
                               \smallskip
                              }

                            
              \end{center}}  %&

\vspace{5cm}

\begin{center}
        {\bf\Huge Applied Reinforcement Learning Project} % Studienarbeit, Interdisziplinäres Projekt
\end{center}

\begin{center}
        \settowidth{\baselineskip}{0.4cm}
        {\LARGE 
		-Title-
	    \vspace*{3.5cm}
        %\LARGE Alperen Gündogan 
        }
\end{center}

\begin{center}
\LARGE Alperen Gündogan, Rachid Ellouze, Uzair Akbar
\end{center}

\begin{center}
\large\today
\end{center}

\vspace*{3.2cm}
\begin{center}
    \epsfxsize=3cm
    \epsfbox{LDVLogoCMYK_oT.pdf}
    \hspace*{9cm}
    \epsfxsize=3cm
    \epsfbox{TUMLogo_oZ_Vollfl_CMYK.pdf}
\end{center}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% MAIN PART:

%\include{Abstract}

\setlang{english}
\thispagestyle{plain}

\section{Motivation}
Some robotic applications in the real world i.e. autonomous driving, shipping in warehouse are difficult to engineer, therefore a agent that exploits reinforcement learning algorithms would become necessary to tame complex models. The RL agent learns from the interactions i.e. experiences with the real world. The agent does not require a predefined model of the problem and a good design of the algorithm leads to a successful result to solve the complex tasks. The key difference of RL from the other learning algorithms is that the RL considers the whole problem during the learning process while other methods e.g. supervised learning only focuses on sub-problems. In this project, we aim to apply deep reinforcement learning algorithm for obstacle avoidance.
* Add something about the algorithm that we will choose, advantages and maybe put some references of other papers.

\section{Goals}
\begin{itemize}
\item The robot can reach the goal position starting from any position.
\item The robot avoids the collision with obstacles during its path.
\item The robot moves smoothly and reaches to the goal position as fast as possible.
\end{itemize}

\section{Project Steps}
\begin{itemize}
\item Get familiar with the turtlebot2, ROS, sensor, libraries.
\item Setup the simulation framework.
\item Implement basic functionalities i.e. movement, rotation, reading sensors etc.
\item Apply different RL approaches containing algorithms, reward signals and states.
\item Validation of the final approach.
\item Final report and presentation.
\end{itemize}

\section{Time Plan}

\begin{figure}
%\begin{center}
%\begin{preview}

\noindent\resizebox{\textwidth}{!}{
%\begin{tikzpicture}[x=.5cm, y=1cm]
    \begin{ganttchart}[
	vgrid={*{6}{draw=none},dotted},  % vgrid, 
    hgrid = true,
    bar height=.2,
    x unit = .07cm,
    y unit title=.6cm,
    y unit chart=.4cm,
    milestone/.append style={fill=green},
    bar/.append style={fill=red},
    time slot format=isodate,
    ]{2019-05-06}{2019-07-18}  % <---
      \gantttitlecalendar{year, month=name}\\
       \ganttgroup{WP-1}{2019-05-06}{2019-05-18} \\
       \ganttgroup{WP-2}{2019-05-18}{2019-05-25}\\
       \ganttbar{T2.1}{2019-05-18}{2019-05-22} \\
       \ganttbar{T2.2}{2019-05-22}{2019-05-25} \\
       \ganttmilestone{MS-1}{2019-05-25}\\
       \ganttgroup{WP-3}{2019-05-25}{2019-06-05}\\
       \ganttbar{T3.1}{2019-05-25}{2019-06-01} \\ 
       \ganttbar{T3.2}{2019-05-25}{2019-06-01} \\
        \ganttbar{T3.3}{2019-05-25}{2019-06-01} \\ 
       \ganttbar{T3.4}{2019-06-01}{2019-06-05} \\
        \ganttmilestone{MS-2}{2019-06-05} \\
       \ganttgroup{WP-4}{2019-06-05}{2019-07-05}  \\
        \ganttbar{T4.1}{2019-06-05}{2019-06-15} \\
         \ganttbar{T4.2}{2019-06-15}{2019-06-25} \\
        \ganttbar{T4.3}{2019-06-25}{2019-07-05} \\   
       \ganttgroup{WP-5}{2019-07-05}{2019-07-10}  \\
       \ganttmilestone{MS-3}{2019-07-10}  \\
       \ganttgroup{WP-6}{2019-07-10}{2019-07-18}  \\
    \end{ganttchart}
}
%\end{center}
%\caption{Gantt Chart}
\end{figure}
%\end{preview}
%%
\begin{itemize}
\item Work Package 1: Tutorials(Alperen, Rachid, Uzair)

	\begin{enumerate}
	\item Practice with ROS, actors, sensors and libraries.
	\end{enumerate}
	
\item Work Package 2: (Alperen, Rachid, Uzair)
	\begin{enumerate}
	\item  Setup of the environment and simulation framework.
	\item Implement basic functionalities.(\textit{Milestone 1}).
	\end{enumerate}
	
\item Work Package 3: Apply a draft algorithm in the simulation environment.
	\begin{enumerate}
	\item Feature extraction design (...)
	\item Reward function and  action space  design (...)
	\item Implement reinforcement learning algorithm.
	\item Tests with the simulation and decide for the parameters i.e. discount factor, learning rate.
	\end{enumerate}
	
\item Work Package 4: Test with the real agent and apply different approaches containing algorithms, reward signals and states.
	\begin{enumerate}
	\item Approach A (i.e. TD Learning with LFVA)
	\item Approach B (SARSA($\lambda$))
	\item Approach C (Q-learning)
	\end{enumerate}
	
\item Work Package 5:  Decide and validate the final approach.

\item Work Package 6: Presentation and final report.


\end{itemize}






% References (Literaturverzeichnis):
% a) Style (with numbers: use unsrt):
\bibliographystyle{ieeetr}
% b) The File:
\bibliography{Bibliography}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
